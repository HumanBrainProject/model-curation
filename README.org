#+title: Model curation process

A python module for the model curation process in the HBP. 

It curates entries from their submission in the Model Catalog by the HBP contributors to their publications in the HBP Knowledge Graph. The curation process includes automatic processes of database updates together with manual editing of entries in Google Spreadsheets and email interactions with the model contributors.

** Schematic of the curation pipeline

[[docs/process.png]]

We detail below the different steps composing the curation pipeline

** Curation steps

*** 1) Fetching data from the Model Catalog app

Steps performed: 

- We fetch the informations from the PostgreSQL database of the Model Catalog app (*TO BE CLEANED UP*).

- We transform the "model-based" set of entries in the Model Catalog to a "version-based" set of entries.

- We search in the Knowledge Graph for the UUID (thanks to =fairgraph=) of the provided entries of all fields in the model (when possible). We associate all entries to this =UUID= in the local database).


#+BEGIN_SRC python
python update_DB Catalog-to-Local
#+END_SRC

If you want to (re)-start from scratch from the information contained in the Model Catalog, use:

#+BEGIN_SRC python
python update_DB Catalog-to-DB-full-rewriting
#+END_SRC

*** 2) Writing the local DB on the spreadsheet

#+BEGIN_SRC python
python update_DB Local-to-Spreadsheet
#+END_SRC

*** 3) Visualize information

See the spreadsheet. You can get this url by typing in the shell:

#+BEGIN_SRC bash
echo $curation_url
#+END_SRC

(this was loaded by the =setting_env_variables.sh= script)

*** 4) Interact with model producers to fix missing fields

Done through emails or [[https://support.humanbrainproject.eu/#ticket/view/my_tickets][Zammad platform]]

*** 5) Update the spreadsheet

Fix missing information, ...

*** 6) Writing the spreadsheet updates on the Local DB

#+BEGIN_SRC python
python update_DB Spreadsheet-to-Local
#+END_SRC

- At that step again, we search in the Knowledge Graph for the UUID (thanks to =fairgraph=) of the provided entries of all fields in the model (when possible). We associate all entries to this =UUID= in the local database).

*** 7) Write to KG

- *Other KG consistency check needed*

#+BEGIN_SRC python
python update_DB Local-to-KG
#+END_SRC

*** 8) Release models in the KG

If a model passes all criteria and the authors wants it to be published. Go the [[https://kg-editor.humanbrainproject.eu/][Knowledge Graph Editor]], search for the desired entry using the "filter" tool. Use the "release" button (shape of a cloud) to release the model.

** Rationale behind the pipeline

*** Transformation from a "model-based" set of entries (in the Model Catalog) to a "version-based" set of entries

The Model Catalog database considers entries which are conceptual models that can have evolving implementation over time. On the other hand, the Knowledge Graph only considers specific model instances with a well-defined implementation that can be potentially released (and therefore should be [[https://www.go-fair.org/fair-principles/][FAIR]]).

The chosen approach therefore duplicates a model across all its versions in the Knowledge Graph. A model with 10 versions in the Model Catalog will therefore have 10 ModelInstances in the Knowledge Graph.

*** Use of a local DB and editing through the Google Spreadsheet

The central database of the pipeline is the local databsae and not the Google Spreadsheet (what could be possible, one would store all data on the Spreadsheet and modify directly from there). The reason for this choice is that this service might disappear or become broken. In that case, another tool could be set up to interact and modify the LocalDb. A command line tool will be available soon (*TO BE DONE*).

Also, having the local database allows simple backups over time (in charge of the curator).

** Model template

The metadata are stored as strings. Either "free" strings or strings corresponding to the UUID in the Knowledge Graph (e.g. the metadata related to the Person Yann Zerlaut has the UUID: =003beed8-1ee8-45ec-8737-785ca6239ef0=).

An empty template is stored in the =model_template.py= file. It reads:
#+BEGIN_SRC python
template = {
    
    "name":"", # a string
    
    "alias":"", # a string
    
    "author":[], # a set of a KG UUIDs

    "description":"", # a string

    "identifier":"", # a KG UUID -> generated during model curation !

    "private":"", # a string either "TRUE" or "FALSE"

    # ------ KG METADATA -------- # 
    "abstraction_level":"", # a KG UUID
    "brain_region":"", # a KG UUID
    "cell_type":["", ""], # a set of KG UUIDs
    "code_format":"", # a string
    "creation_date":"", # a KG UUID
    "license":"", # a KG UUID
    "model_scope":"", # a KG UUID
    "model_type":"", # a KG UUID
    "organization":"", # a KG UUID
    "owner":"", # a KG UUID
    "pla_components":"", # a KG UUID
    "project":"", # a KG UUID
    "associated_dataset":[], # a KG UUID
    "associated_method":[], # a KG UUID
    "associated_experimental_preparation":[], # a set of KG UUIDs
    "used_software":[],
    
    # ------ VERSIONS -------- # 
    "version":[
	{"source":"",
	 "name":""},
	{"source":"",
	 "name":""},
    ],
    
    # ------ VERSIONS -------- # 
    "images":[
	{"url":"",
	 "caption":""}
    ],
}    
#+END_SRC

** Use of scripts


run the =setting_env_variables.sh=  script in the shell 

#+BEGIN_SRC bash
cd folder_where_you_have_cloned_the_repo/model-curation/
source setting_env_variables.sh
#+END_SRC 

** Dependencies

Two python modules of the Human Brain Project ecosystem:

- [[https://github.com/HumanBrainProject/fairgraph][fairgraph]]: A high-level Python API for the HBP Knowledge Graph
- [[https://github.com/HumanBrainProject/hbp-validation-client][hbp-validation-client]]: A Python package for working with the Human Brain Project Model Validation Framework.

The Python API for working with Google Spreadsheets:

- [[https://developers.google.com/sheets/api][Google Spreadsheet API]]

Follow the instructions to get the credentials at:

https://developers.google.com/sheets/api/quickstart/python

** Configuration file

A file 
#+BEGIN_SRC python
import os

# location of your json files for the HBP logins, as a python path
hbp_token_file=os.path.join(os.path.expanduser('~'), 'Downloads', 'HBP.json')
hbp_storage_token_file=os.path.join(os.path.expanduser('~'), 'Downloads', 'config.json')

# Google spreadsheet credential logins
...

# ID of Google Spreadsheets 
SGA2_SP6_SPREADSHEET_ID= '...' 
SGA2_SP3_SPREADSHEET_ID='...'


#+END_SRC python

** Stats

A detailed analysis of the curation pipeline is available at:

https://github.com/yzerlaut/model-curation/blob/master/stats/summary.org

